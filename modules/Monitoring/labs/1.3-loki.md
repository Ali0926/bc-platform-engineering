
# Lab: Centralized Logging with Loki

## Overview

In this lab, you'll experience the power of centralized logging. You already know how to use `docker logs` to see what a container is doing—but what happens when you have 10 containers? 100? What if you need to search logs from last week?

**Loki** is a log aggregation system designed to work seamlessly with Grafana. Think of it as "Prometheus, but for logs." You'll learn to query logs using **LogQL**, Loki's query language, and build dashboards that give you real-time visibility into your applications.

---

## Part 1: The Problem with `docker logs`

Before we set up centralized logging, let's understand *why* we need it.

### The CLI Approach

When you run a container, you can see its logs:

```bash
docker logs my-container
```

Need to filter? Use grep:

```bash
docker logs my-container | grep ERROR
```

Follow logs in real-time:

```bash
docker logs -f my-container
```

### Why This Breaks Down

| Scenario | CLI Problem |
|----------|-------------|
| Multiple containers | Need separate terminal for each |
| Container restarts | Logs are lost (unless configured) |
| Searching old logs | No easy way to query historical data |
| Correlating events | Can't easily see what happened across services |
| Team collaboration | Everyone needs SSH access |

Centralized logging solves all of these by shipping logs to a central location where they can be queried, visualized, and retained.

---

## Part 2: The Logging Stack

We'll deploy four services that work together:

| Service | Role | Analogy |
|---------|------|---------|
| **API** | Your application that generates logs | The source |
| **Promtail** | Collects logs and sends to Loki | The shipper |
| **Loki** | Stores and indexes logs | The database |
| **Grafana** | Visualizes and queries logs | The UI |

```
┌─────────┐     ┌──────────┐     ┌──────┐     ┌─────────┐
│   API   │────▶│ Promtail │────▶│ Loki │◀────│ Grafana │
└─────────┘     └──────────┘     └──────┘     └─────────┘
   logs           collect         store         query
```

### Setting Up the Stack

Create a directory for this lab:

```bash
mkdir ~/loki-lab
cd ~/loki-lab
```

We'll use Docker Compose to run all services together. Create the following files:

**docker-compose.yml** - Defines all our services:

```yaml
services:
  api:
    build: ./api
    container_name: telemetry-api
    ports:
      - "8000:8000"
    networks:
      - monitoring
    labels:
      - "app=telemetry-api"

  promtail:
    image: grafana/promtail:3.2.1
    container_name: promtail
    command: ["-config.file=/etc/promtail/promtail-config.yml"]
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./promtail-config.yaml:/etc/promtail/promtail-config.yml
    networks:
      - monitoring
    depends_on:
      - loki

  loki:
    image: grafana/loki:2.9.0
    container_name: loki
    ports:
      - "3100:3100"
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - ./datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    networks:
      - monitoring
    depends_on:
      - loki
    labels:
      - "app=grafana"

networks:
  monitoring:
    driver: bridge
```

**promtail-config.yaml** - Tells Promtail how to collect Docker logs:

```yaml
server:
  http_listen_port: 9080

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: docker
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
    relabel_configs:
      - source_labels: ['__meta_docker_container_name']
        target_label: 'container'
        regex: '/(.*)'
      - source_labels: ['__meta_docker_container_label_app']
        target_label: 'app'
```

**datasources.yaml** - Auto-configures Loki in Grafana:

```yaml
apiVersion: 1

datasources:
  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    isDefault: true
```

**api/Dockerfile** - Builds your API container (If Needed):

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY main.py .
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**api/requirements.txt**:

```txt
fastapi==0.109.0
uvicorn==0.27.0
prometheus-fastapi-instrumentator==6.1.0
```

**api/main.py** - Copy the API from Lab 1.2.

### Start the Stack

```bash
docker compose up -d --build
```

Verify all containers are running:

```bash
docker ps
```

You should see four containers: `telemetry-api`, `promtail`, `loki`, and `grafana`.

---

## Part 3: CLI Logging vs. Grafana

Now let's compare the traditional CLI approach with centralized logging.

### Generate Some Logs

First, let's create some activity. Hit the API endpoints:

```bash
# Successful requests
curl http://localhost:8000/
curl http://localhost:8000/items/1
curl http://localhost:8000/items/2

# Some errors
curl http://localhost:8000/items/999
curl http://localhost:8000/slow

# Generate random behavior
for i in {1..20}; do
  curl -s http://localhost:8000/random > /dev/null
  sleep 0.5
done
```

### The CLI Way

View all logs from the API:

```bash
docker logs telemetry-api
```

You'll see output like:

```
2026-01-18 10:15:23 - INFO - telemetry-api - Application starting up
2026-01-18 10:15:45 - INFO - telemetry-api - Root endpoint accessed
2026-01-18 10:15:46 - INFO - telemetry-api - Fetching item id=1
2026-01-18 10:15:47 - WARNING - telemetry-api - Item not found: 999
2026-01-18 10:15:48 - DEBUG - telemetry-api - Slow operation starting
```

### The Grafana Way

1. Open Grafana: http://localhost:3000
2. Login with `admin` / `admin`
3. Click **Explore** in the left sidebar (compass icon)
4. Make sure **Loki** is selected as the data source

In the query builder:
1. Click **Select label** and choose `container`
2. Click **Select value** and choose `telemetry-api`
3. Click **Run query**

You should see the same logs—but now in a searchable, visual interface!

---

## Part 4: From grep to LogQL

LogQL is Loki's query language. If you know grep, you already understand the basics.

### Finding Errors

**CLI approach:**

```bash
docker logs telemetry-api | grep ERROR
```

**LogQL equivalent:**

```json
{container="telemetry-api"} |= "ERROR"
```

Try it in Grafana's Explore view. The `|=` operator means "line contains."

### Finding Warnings

**CLI:**

```bash
docker logs telemetry-api | grep WARNING
```

**LogQL:**

```logql
{container="telemetry-api"} |= "WARNING"
```

### Case-Insensitive Search

**CLI:**

```bash
docker logs telemetry-api | grep -i error
```

**LogQL:**

```logql
{container="telemetry-api"} |~ "(?i)error"
```

The `|~` operator enables regex matching.

### Excluding Lines

**CLI:**

```bash
docker logs telemetry-api | grep -v DEBUG
```

**LogQL:**

```logql
{container="telemetry-api"} != "DEBUG"
```

### Multiple Filters (AND)

**CLI:**

```bash
docker logs telemetry-api | grep INFO | grep item
```

**LogQL:**

```logql
{container="telemetry-api"} |= "INFO" |= "item"
```

### Exercise: LogQL Practice

Generate more traffic and try these queries:

1. Find all log lines containing "Fetching"
2. Find WARNING or ERROR logs (hint: use regex with `|`)
3. Find logs about item id=1 specifically
4. Exclude all DEBUG logs, then filter for "slow"

<details>
<summary>Solutions</summary>

```logql
# 1. Lines containing "Fetching"
{container="telemetry-api"} |= "Fetching"

# 2. WARNING or ERROR
{container="telemetry-api"} |~ "WARNING|ERROR"

# 3. Item id=1
{container="telemetry-api"} |= "id=1"

# 4. Exclude DEBUG, find "slow"
{container="telemetry-api"} != "DEBUG" |= "slow"
```

</details>

---

## Cleanup

When you're done exploring, stop the containers:

```bash
docker compose down
```

---

## Summary

| Concept | What You Learned |
|---------|------------------|
| **Centralized Logging** | Aggregate logs from multiple containers in one place |
| **Loki** | Stores and indexes logs efficiently |
| **Promtail** | Ships container logs to Loki |
| **LogQL Basics** | `{labels}` to select, `\|=` to filter |

### LogQL Quick Reference

| Operation | LogQL | grep Equivalent |
|-----------|-------|-----------------|
| Contains | `\|= "text"` | `grep "text"` |
| Not contains | `!= "text"` | `grep -v "text"` |
| Regex match | `\|~ "pattern"` | `grep -E "pattern"` |
| Case insensitive | `\|~ "(?i)text"` | `grep -i "text"` |

---

## Next Steps

In the next lab, you'll add **Prometheus** to the stack to collect metrics from your API's `/metrics` endpoint. You'll see how metrics and logs complement each other—logs tell you *what* happened, metrics tell you *how much* and *how often*.


