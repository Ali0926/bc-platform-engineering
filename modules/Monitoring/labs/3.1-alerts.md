# Lab: Alerting with Grafana, ArgoCD, and Slack

## Overview

In this lab, you'll set up real-time alerts for your Kubernetes environment using Grafana, Prometheus, and Slack. You'll fork the monitoring repo, configure a Slack webhook, create an alert for MySQL availability, and simulate a failure to see your alert in action.

---

## Part 1: Fork the Monitoring Repository

1. Go to [https://github.com/d-blanco/k8s-argo-monitoring](https://github.com/d-blanco/k8s-argo-monitoring)
2. Click **Fork** to create your own copy under your GitHub account.
3. Clone your fork locally:
	```bash
	git clone https://github.com/<your-username>/k8s-argo-monitoring.git
	cd k8s-argo-monitoring
	```
 4. Go to ArgoCD UI and update the main app to point to your fork

---

## Part 2: Set Up Slack Webhook

1. In your Slack workspace, create a new app (or use an existing one).
2. Add the **Incoming Webhooks** feature to your app.
3. Create a new webhook and select the channel where you want to receive alerts.
4. Copy the webhook URL (it will look like `https://hooks.slack.com/services/...`).

---

## Part 3: Create a Grafana Alert for MySQL Availability

1. Open Grafana (from your monitoring stack).
2. Go to your dashboard or create a new one.
3. Add a **Stat** or **Time series** panel with the following Prometheus query:
	```promql
	mysql_up
	```
	This metric is `1` when MySQL is up, `0` when down.
4. Click the **Alert** tab and create a new alert rule:
	- **Condition:** When `mysql_up` is `0` for 1 minute
	- **Evaluate every:** 1m
	- **For:** 1m
	- **No Data/Null:** Treat as alerting
5. In **Contact points**, add a new contact point for **Slack**:
	- Type: **Slack**
	- Webhook URL: (paste your Slack webhook URL)
	- Channel: (optional, if not set in webhook)
6. Assign the contact point to your alert rule.
7. Save the dashboard and alert rule.

---

## Part 4: Simulate a MySQL Outage with ArgoCD Pruning

1. In your forked repo, navigate to the MySQL deployment folder:
	```bash
	cd apps/mysql
	```
2. Rename the deployment manifest so ArgoCD will prune the resource:
	```bash
	git mv deployment.yaml deployment.yaml.tmp
	git commit -am "Simulate MySQL outage by removing deployment"
	git push
	```
3. Update the apps/argocd-apps/mysql.yaml file to use the repo URL in your fork
4. Wait for ArgoCD to sync. The MySQL deployment will be deleted from the cluster.
5. Watch your Slack channel for the alert from Grafana!

---

## Part 5: Restore MySQL (Cleanup)

1. Rename the file back to restore MySQL:
	```bash
	git mv deployment.yaml.tmp deployment.yaml
	git commit -am "Restore MySQL deployment"
	git push
	```
2. ArgoCD will recreate the deployment and the alert should clear.

---

## Summary

- You set up a real alert pipeline from Grafana to Slack.
- You simulated a real outage and saw the alert fire.
- You learned how GitOps and monitoring work together for rapid incident response.

---

## Challenge

- Try creating additional alerts for other critical resources (e.g., API, Prometheus, node exporter).
- Experiment with different alert conditions and notification channels.
