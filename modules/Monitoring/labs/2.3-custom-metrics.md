# Lab: Custom Metrics with Prometheus

## Overview

In the previous lab, you built PromQL queries using the **built-in metrics** from `prometheus-fastapi-instrumentator`. Now you'll learn to create your own **custom metrics** to track business-specific data that the instrumentator doesn't capture automatically.

Your stack from Lab 1.6 should still be running. If not, start it:

```bash
cd ~/loki-lab
docker compose up -d
```

---

## Part 1: When to Use Custom Metrics

The built-in instrumentator gives you:
- `http_requests_total` - request counts
- `http_request_duration_seconds` - latency histograms
- `http_requests_in_progress` - active requests

But what about:
- How many items were processed successfully vs failed?
- What's the current queue depth?
- How long does business logic take (not just HTTP latency)?

That's where custom metrics come in.

| Metric Type | Use Case | Example |
|-------------|----------|---------|
| **Counter** | Cumulative totals that only increase | Items processed, errors logged |
| **Gauge** | Values that go up and down | Queue depth, active users |
| **Histogram** | Distribution of values | Processing time, payload size |

---

## Part 2: Add Custom Metrics to the API

### Update main.py

Replace your `api/main.py` with this enhanced version that includes custom metrics:

```python
"""
Telemetry API with Custom Prometheus Metrics
"""

import logging
import random
import time
from fastapi import FastAPI, HTTPException
from prometheus_fastapi_instrumentator import Instrumentator
from prometheus_client import Counter, Gauge, Histogram

# Configure logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s'
)
logger = logging.getLogger("telemetry-api")

# Create FastAPI app
app = FastAPI(
    title="Telemetry API",
    description="API with custom Prometheus metrics",
    version="2.0.0"
)

# Built-in instrumentation
Instrumentator().instrument(app).expose(app)

# ============================================
# CUSTOM METRICS
# ============================================

# Counter: Track items processed with success/failure labels
items_processed = Counter(
    'items_processed_total',
    'Total number of items processed',
    ['status']  # Label to distinguish success/failure
)

# Gauge: Track simulated queue depth
queue_depth = Gauge(
    'processing_queue_depth',
    'Current number of items waiting to be processed'
)

# Histogram: Track business logic processing time
processing_duration = Histogram(
    'item_processing_duration_seconds',
    'Time spent on business logic processing',
    buckets=[0.1, 0.25, 0.5, 1.0, 2.0, 5.0]
)

# Counter: Track API usage by endpoint category
endpoint_hits = Counter(
    'endpoint_hits_total',
    'Hits by endpoint category',
    ['category']
)

# Gauge: Simulated system health score
health_score = Gauge(
    'system_health_score',
    'Simulated system health from 0-100'
)

# Initialize queue with some items
queue_depth.set(5)
health_score.set(100)

# ============================================
# DATA STORE
# ============================================

items_db = {
    1: {"id": 1, "name": "Widget", "status": "active"},
    2: {"id": 2, "name": "Gadget", "status": "active"},
    3: {"id": 3, "name": "Gizmo", "status": "inactive"},
}

# ============================================
# ENDPOINTS
# ============================================

@app.get("/")
def root():
    """Health check endpoint"""
    endpoint_hits.labels(category='health').inc()
    logger.info("Health check requested")
    return {"status": "healthy", "service": "telemetry-api"}


@app.get("/items")
def get_items():
    """Get all items"""
    endpoint_hits.labels(category='read').inc()
    logger.info(f"Fetching all items, count: {len(items_db)}")
    return {"items": list(items_db.values())}


@app.get("/items/{item_id}")
def get_item(item_id: int):
    """Get single item"""
    endpoint_hits.labels(category='read').inc()
    
    if item_id not in items_db:
        logger.warning(f"Item not found: {item_id}")
        raise HTTPException(status_code=404, detail="Item not found")
    
    logger.info(f"Retrieved item: {items_db[item_id]['name']}")
    return items_db[item_id]


@app.post("/items/{item_id}/process")
def process_item(item_id: int):
    """
    Process an item - demonstrates custom metrics
    """
    endpoint_hits.labels(category='write').inc()
    logger.info(f"Starting to process item: {item_id}")
    
    if item_id not in items_db:
        logger.error(f"Cannot process non-existent item: {item_id}")
        items_processed.labels(status='not_found').inc()
        raise HTTPException(status_code=404, detail="Item not found")
    
    # Simulate adding to queue
    queue_depth.inc()
    
    # Time the business logic with our custom histogram
    start_time = time.time()
    
    # Simulate processing (200ms to 3 seconds)
    delay = random.uniform(0.2, 3.0)
    time.sleep(delay)
    
    # Record the processing duration
    processing_duration.observe(time.time() - start_time)
    
    # Simulate occasional failures (20% chance)
    if random.random() < 0.2:
        logger.error(f"Processing failed for item {item_id}")
        items_processed.labels(status='failed').inc()
        queue_depth.dec()
        # Degrade health score on failure
        health_score.set(max(0, health_score._value._value - 5))
        raise HTTPException(status_code=500, detail="Processing failed")
    
    # Success
    items_processed.labels(status='success').inc()
    queue_depth.dec()
    # Restore health score on success
    health_score.set(min(100, health_score._value._value + 1))
    
    logger.info(f"Successfully processed item: {item_id}")
    return {"status": "processed", "item_id": item_id, "duration": delay}


@app.get("/slow")
def slow_endpoint():
    """Deliberately slow endpoint"""
    endpoint_hits.labels(category='slow').inc()
    delay = random.uniform(1.0, 5.0)
    logger.debug(f"Slow operation: {delay:.2f}s")
    time.sleep(delay)
    return {"waited": delay}


@app.get("/random")
def random_endpoint():
    """Random behavior endpoint"""
    endpoint_hits.labels(category='random').inc()
    
    if random.random() < 0.3:
        logger.error("Random failure triggered")
        raise HTTPException(status_code=500, detail="Random error")
    
    return {"random": random.randint(1, 100)}


@app.get("/queue/add")
def add_to_queue():
    """Simulate adding items to queue"""
    endpoint_hits.labels(category='queue').inc()
    items_to_add = random.randint(1, 5)
    queue_depth.inc(items_to_add)
    logger.info(f"Added {items_to_add} items to queue")
    return {"added": items_to_add, "queue_depth": queue_depth._value._value}
```

### Rebuild and Restart

```bash
docker compose up -d --build
```

Wait a few seconds, then verify the new metrics are available:

```bash
curl -s http://localhost:8000/metrics | grep -E "^(items_processed|queue_depth|processing_duration|endpoint_hits|health_score)"
```

You should see your custom metrics listed.

---

## Part 3: Generate Traffic with Custom Metrics

Run this script to generate varied traffic that exercises the custom metrics:

```bash
for i in {1..60}; do
  # Read operations
  curl -s http://localhost:8000/items > /dev/null
  curl -s http://localhost:8000/items/1 > /dev/null
  curl -s http://localhost:8000/items/999 > /dev/null  # 404
  
  # Processing (creates counter + histogram data)
  curl -s -X POST http://localhost:8000/items/1/process > /dev/null
  curl -s -X POST http://localhost:8000/items/2/process > /dev/null
  
  # Queue simulation
  curl -s http://localhost:8000/queue/add > /dev/null
  
  # Random endpoint
  curl -s http://localhost:8000/random > /dev/null
  
  echo "Iteration $i/60"
  sleep 1
done
```

Let this run while you explore the queries in the next section.

---

## Part 4: PromQL Queries for Custom Metrics

Open Prometheus at http://localhost:9090 and try these queries.

### Counter Queries

**Total items processed (all statuses):**
```promql
items_processed_total
```

**Items processed by status:**
```promql
sum by (status) (items_processed_total)
```

**Processing success rate:**
```promql
sum(items_processed_total{status="success"})
/
sum(items_processed_total)
```

**Processing rate per second by status:**
```promql
sum by (status) (rate(items_processed_total[5m]))
```

**Endpoint hits by category:**
```promql
sum by (category) (rate(endpoint_hits_total[5m]))
```

### Gauge Queries

**Current queue depth:**
```promql
processing_queue_depth
```

**Current health score:**
```promql
system_health_score
```

**Average health score over time:**
```promql
avg_over_time(system_health_score[10m])
```

### Histogram Queries

**95th percentile processing duration:**
```promql
histogram_quantile(0.95, 
  sum by (le) (rate(item_processing_duration_seconds_bucket[5m]))
)
```

**50th percentile (median) processing duration:**
```promql
histogram_quantile(0.50, 
  sum by (le) (rate(item_processing_duration_seconds_bucket[5m]))
)
```

**Percentage of items processed under 1 second:**
```promql
sum(rate(item_processing_duration_seconds_bucket{le="1"}[5m]))
/
sum(rate(item_processing_duration_seconds_bucket{le="+Inf"}[5m]))
* 100
```

---

## Part 5: SLO Queries with Custom Metrics

Now let's define SLOs using our custom metrics.

### SLO 1: Processing Success Rate ≥ 80%

```promql
sum(rate(items_processed_total{status="success"}[5m]))
/
sum(rate(items_processed_total{status!="not_found"}[5m]))
* 100
```

We exclude `not_found` because that's a client error, not our failure.

### SLO 2: 95% of Processing Under 2 Seconds

```promql
sum(rate(item_processing_duration_seconds_bucket{le="2"}[5m]))
/
sum(rate(item_processing_duration_seconds_bucket{le="+Inf"}[5m]))
* 100
```

### SLO 3: Health Score Above 80

```promql
system_health_score >= 80
```

Returns 1 if true, 0 if false. Useful for alerting.

### SLO 4: Queue Depth Under 20

```promql
processing_queue_depth < 20
```

---

## Part 6: Combining Built-in and Custom Metrics

The power of observability comes from correlating different metrics.

### Error Correlation

Compare HTTP 500s (built-in) with processing failures (custom):

**HTTP 500 rate:**
```promql
sum(rate(http_requests_total{status="5xx"}[5m]))
```

**Processing failure rate:**
```promql
sum(rate(items_processed_total{status="failed"}[5m]))
```

### Latency Correlation

Compare HTTP latency (built-in) with business processing time (custom):

**HTTP P95 for /process endpoint:**
```promql
histogram_quantile(0.95, 
  sum by (le) (rate(http_request_duration_seconds_bucket{handler="/items/{item_id}/process"}[5m]))
)
```

**Business logic P95:**
```promql
histogram_quantile(0.95, 
  sum by (le) (rate(item_processing_duration_seconds_bucket[5m]))
)
```

The difference between these shows overhead from HTTP handling.

---

## Part 7: Dashboard Exercise

Add panels to your existing SLO dashboard from Lab 1.6 for the custom metrics:

### Panel: Processing Success Rate

**Query:**
```promql
sum(rate(items_processed_total{status="success"}[5m]))
/
sum(rate(items_processed_total{status!="not_found"}[5m]))
* 100
```

**Settings:**
- Visualization: Gauge
- Unit: Percent (0-100)
- Thresholds: Red < 75, Yellow < 85, Green ≥ 85

### Panel: Queue Depth

**Query:**
```promql
processing_queue_depth
```

**Settings:**
- Visualization: Stat
- Thresholds: Green < 10, Yellow < 20, Red ≥ 20

### Panel: Health Score

**Query:**
```promql
system_health_score
```

**Settings:**
- Visualization: Gauge
- Min: 0, Max: 100
- Thresholds: Red < 50, Yellow < 80, Green ≥ 80

### Panel: Processing Duration P95

**Query:**
```promql
histogram_quantile(0.95, 
  sum by (le) (rate(item_processing_duration_seconds_bucket[5m]))
)
```

**Settings:**
- Visualization: Stat
- Unit: seconds

---

## Summary

| Metric Type | When to Use | Query Pattern |
|-------------|-------------|---------------|
| **Counter** | Cumulative events | `sum by (label) (rate(counter[5m]))` |
| **Gauge** | Current values | Direct query, `avg_over_time()` |
| **Histogram** | Distributions | `histogram_quantile(percentile, sum by (le) (...))` |

### Custom Metrics You Created

| Metric | Type | Purpose |
|--------|------|---------|
| `items_processed_total` | Counter | Track success/failure by label |
| `processing_queue_depth` | Gauge | Current queue size |
| `item_processing_duration_seconds` | Histogram | Business logic latency |
| `endpoint_hits_total` | Counter | Categorize endpoint usage |
| `system_health_score` | Gauge | Simulated health metric |

### Key PromQL Patterns

**Success rate from counter:**
```promql
sum(rate(counter{status="success"}[5m])) / sum(rate(counter[5m]))
```

**Percentile from histogram:**
```promql
histogram_quantile(0.95, sum by (le) (rate(histogram_bucket[5m])))
```

**Percentage under threshold:**
```promql
sum(rate(histogram_bucket{le="THRESHOLD"}[5m])) / sum(rate(histogram_bucket{le="+Inf"}[5m]))
```

---

## Cleanup

When done:

```bash
docker compose down
```

---

## Next Steps

In the next lab, you'll explore LogQL in depth to parse structured logs and correlate them with your metrics.
