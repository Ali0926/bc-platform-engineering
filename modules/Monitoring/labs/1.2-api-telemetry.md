
# Lab: Deploying an API with Telemetry

## Overview
In this lab, you'll deploy a FastAPI application that supports **telemetry**—the ability to emit data about itself that monitoring tools can integrate with. You'll instrument the API with Prometheus metrics and structured logging, then observe how this data becomes available to tools like Grafana, Prometheus, and Loki.

---

## Part 1: What is Telemetry?

Telemetry is data that an application emits about its own behavior. Think of it as your application "reporting" on itself.

| Telemetry Type | What It Tells You | Example |
|----------------|-------------------|---------|
| **Metrics** | Numeric measurements | Request count, response time, error rate |
| **Logs** | Event records | "User login failed", "Database connection timeout" |
| **Traces** | Request flow | How a request travels through services |

### Why Telemetry Matters

Without telemetry:
- You only know something is wrong when users complain
- Debugging is guesswork

With telemetry:
- You see problems as they happen
- You understand *why* things fail
- Monitoring tools can alert you proactively

---

## Part 2: The FastAPI Application

We'll build a simple API that:
- Exposes Prometheus metrics at `/metrics`
- Logs at different levels (DEBUG, INFO, WARNING, ERROR)
- Has endpoints that simulate real-world behavior

### Project Structure

Create a new directory for your project:

```bash
mkdir ~/telemetry-api
cd ~/telemetry-api
```

### Step 1: Create `requirements.txt`

```txt
fastapi==0.109.0
uvicorn==0.27.0
prometheus-fastapi-instrumentator==6.1.0
```

### Step 2: Create `main.py`

```python
"""
Telemetry API - A FastAPI application with Prometheus metrics and structured logging

This application demonstrates:
- Prometheus metrics instrumentation
- Logging at different levels
- Endpoints that monitoring tools can integrate with
"""

import logging
import random
import time
from fastapi import FastAPI, HTTPException
from prometheus_fastapi_instrumentator import Instrumentator

# Configure logging with different levels
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s'
)
logger = logging.getLogger("telemetry-api")

# Create FastAPI app
app = FastAPI(
    title="Telemetry API",
    description="An API that emits metrics and logs for observability",
    version="1.0.0"
)

# Instrument with Prometheus metrics
# This automatically tracks request count, latency, and more
Instrumentator().instrument(app).expose(app)

# Simulated data store
items_db = {
    1: {"id": 1, "name": "Widget", "status": "active"},
    2: {"id": 2, "name": "Gadget", "status": "active"},
    3: {"id": 3, "name": "Gizmo", "status": "inactive"},
}


@app.get("/")
def root():
    """Health check endpoint"""
    logger.info("Health check requested")
    return {"status": "healthy", "service": "telemetry-api"}


@app.get("/items")
def get_items():
    """Get all items - logs at INFO level"""
    logger.info(f"Fetching all items, count: {len(items_db)}")
    return {"items": list(items_db.values())}


@app.get("/items/{item_id}")
def get_item(item_id: int):
    """Get single item - demonstrates INFO and WARNING logs"""
    logger.debug(f"Looking up item with id: {item_id}")
    
    if item_id not in items_db:
        logger.warning(f"Item not found: {item_id}")
        raise HTTPException(status_code=404, detail="Item not found")
    
    item = items_db[item_id]
    logger.info(f"Retrieved item: {item['name']}")
    return item


@app.post("/items/{item_id}/process")
def process_item(item_id: int):
    """
    Process an item - simulates work with random delays and occasional failures
    Great for generating interesting metrics and logs!
    """
    logger.info(f"Starting to process item: {item_id}")
    
    if item_id not in items_db:
        logger.error(f"Cannot process non-existent item: {item_id}")
        raise HTTPException(status_code=404, detail="Item not found")
    
    # Simulate processing time (200ms to 2 seconds)
    delay = random.uniform(0.2, 2.0)
    logger.debug(f"Processing will take {delay:.2f} seconds")
    time.sleep(delay)
    
    # Simulate occasional failures (20% chance)
    if random.random() < 0.2:
        logger.error(f"Processing failed for item {item_id} - simulated error")
        raise HTTPException(status_code=500, detail="Processing failed")
    
    logger.info(f"Successfully processed item: {item_id}")
    return {"item_id": item_id, "status": "processed", "duration": f"{delay:.2f}s"}


@app.get("/slow")
def slow_endpoint():
    """Intentionally slow endpoint - useful for testing latency metrics"""
    delay = random.uniform(1.0, 5.0)
    logger.warning(f"Slow endpoint called, sleeping for {delay:.2f}s")
    time.sleep(delay)
    logger.info("Slow endpoint completed")
    return {"message": "Finally done!", "delay": f"{delay:.2f}s"}


@app.get("/error")
def error_endpoint():
    """Always fails - useful for testing error metrics and logs"""
    logger.error("Error endpoint called - this will fail intentionally")
    raise HTTPException(status_code=500, detail="Intentional error for testing")


@app.get("/debug")
def debug_endpoint():
    """Generates lots of debug logs"""
    logger.debug("Debug endpoint called")
    logger.debug(f"Current items in database: {len(items_db)}")
    logger.debug(f"Item IDs: {list(items_db.keys())}")
    logger.info("Debug information generated")
    return {"debug": "Check the logs for debug output"}


@app.get("/random")
def random_endpoint():
    """Generates a random log level - useful for testing log aggregation"""
    log_type = random.choice(["debug", "info", "warning", "error"])
    
    if log_type == "debug":
        logger.debug("Random debug message generated")
    elif log_type == "info":
        logger.info("Random info message generated")
    elif log_type == "warning":
        logger.warning("Random warning message generated")
    else:
        logger.error("Random error message generated")
    
    return {"log_type": log_type, "message": f"Generated a {log_type} log"}
```

### Step 3: Create `Dockerfile`

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Copy requirements first for better caching
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY main.py .

# Expose the API port
EXPOSE 8000

# Run the application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## Part 3: Build and Deploy the Image

You have two options to get your image ready:

### Option A: Build and Push Manually

```bash
# Build the image
docker build -t telemetry-api:latest .

# Tag for your registry (Docker Hub example)
docker tag telemetry-api:latest yourusername/telemetry-api:latest

# Push to registry
docker push yourusername/telemetry-api:latest
```

### Option B: Use a CI/CD Pipeline

If you have a CI/CD pipeline set up (GitHub Actions, Jenkins, GitLab CI), push your code and let the pipeline build and push the image for you.

**The choice is yours—figure out which approach works for your environment!**

---

## Part 4: Run the Application

Once your image is available, run it on the same host as Grafana:

```bash
# If you built locally
docker run -d --name telemetry-api -p 8000:8000 telemetry-api:latest

# Or if you pushed to a registry
docker run -d --name telemetry-api -p 8000:8000 yourusername/telemetry-api:latest
```

Verify it's running:

```bash
docker ps
```

You should see both Grafana and your API:

```
CONTAINER ID   IMAGE                      PORTS                    NAMES
abc123         grafana/grafana            0.0.0.0:3000->3000/tcp   grafana
def456         telemetry-api:latest       0.0.0.0:8000->8000/tcp   telemetry-api
```

---

## Part 5: Explore the API

### The `/docs` Endpoint - Interactive API Documentation

FastAPI automatically generates interactive documentation. Open your browser:

```
http://localhost:8000/docs
```

You'll see Swagger UI where you can:
- View all available endpoints
- Test endpoints directly in the browser
- See request/response schemas

### The `/metrics` Endpoint - Prometheus Metrics

This is where the telemetry magic happens! Visit:

```
http://localhost:8000/metrics
```

Or use curl:

```bash
curl http://localhost:8000/metrics
```

You'll see output like:

```
# HELP http_requests_total Total number of HTTP requests
# TYPE http_requests_total counter
http_requests_total{handler="/",method="GET",status="2xx"} 1.0
http_requests_total{handler="/items",method="GET",status="2xx"} 3.0

# HELP http_request_duration_seconds HTTP request duration in seconds
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{handler="/",le="0.005",method="GET"} 1.0
...
```

**This is Prometheus format!** Monitoring tools like Prometheus can scrape this endpoint and store these metrics.

### Test with curl

Generate some traffic and see different responses:

```bash
# Health check
curl http://localhost:8000/

# Get all items
curl http://localhost:8000/items

# Get a specific item
curl http://localhost:8000/items/1

# Get a non-existent item (404)
curl http://localhost:8000/items/999

# Process an item (may succeed or fail randomly)
curl -X POST http://localhost:8000/items/1/process

# Hit the slow endpoint
curl http://localhost:8000/slow

# Trigger an error
curl http://localhost:8000/error
```

---

## Part 6: View Application Logs

Now see what your application is telling you:

```bash
docker logs telemetry-api
```

You'll see logs at different levels:

```
2026-01-17 10:00:01 - INFO - telemetry-api - Health check requested
2026-01-17 10:00:05 - INFO - telemetry-api - Fetching all items, count: 3
2026-01-17 10:00:10 - DEBUG - telemetry-api - Looking up item with id: 1
2026-01-17 10:00:10 - INFO - telemetry-api - Retrieved item: Widget
2026-01-17 10:00:15 - WARNING - telemetry-api - Item not found: 999
2026-01-17 10:00:20 - ERROR - telemetry-api - Processing failed for item 1 - simulated error
2026-01-17 10:00:25 - WARNING - telemetry-api - Slow endpoint called, sleeping for 3.45s
```

### Understanding Log Levels

| Level | When to Use | Example |
|-------|-------------|---------|
| DEBUG | Detailed diagnostic info | "Looking up item with id: 1" |
| INFO | Normal operations | "Retrieved item: Widget" |
| WARNING | Something unexpected but handled | "Item not found: 999" |
| ERROR | Something failed | "Processing failed for item 1" |

Follow logs in real-time while testing:

```bash
docker logs -f telemetry-api
```

---

## Part 7: Generate Traffic for Interesting Data

Run this loop to generate varied traffic:

```bash
# Generate 50 random requests
for i in {1..50}; do
    # Random endpoint selection
    case $((RANDOM % 6)) in
        0) curl -s http://localhost:8000/ > /dev/null ;;
        1) curl -s http://localhost:8000/items > /dev/null ;;
        2) curl -s http://localhost:8000/items/$((RANDOM % 5)) > /dev/null ;;
        3) curl -s -X POST http://localhost:8000/items/$((RANDOM % 3 + 1))/process > /dev/null ;;
        4) curl -s http://localhost:8000/slow > /dev/null ;;
        5) curl -s http://localhost:8000/error > /dev/null ;;
    esac
    echo "Request $i sent"
    sleep 0.5
done
```

Now check `/metrics` again—you'll see the counters have increased!

---

## Part 8: Understanding Telemetry Integration

Your API now supports telemetry because it **emits data about itself** that other monitoring tools can integrate with:

| Data Emitted | Format | Tool That Uses It |
|--------------|--------|-------------------|
| `/metrics` endpoint | Prometheus format | Prometheus, Grafana |
| stdout/stderr logs | Text with levels | Loki, Fluentd, Logstash |
| (Future) Traces | OpenTelemetry | Tempo, Jaeger |

### The Observability Pipeline

```
┌─────────────────┐
│  Your API       │
│  - /metrics     │──────────────▶ Prometheus ──▶ Grafana Dashboards
│  - stdout logs  │──────────────▶ Loki ────────▶ Grafana Explore
└─────────────────┘
```

In upcoming labs, you'll:
1. **Connect Prometheus** to scrape the `/metrics` endpoint
2. **Connect Loki** to collect and query the logs
3. **Build dashboards** that visualize this telemetry data

---

## Summary

| What You Did | Why It Matters |
|--------------|----------------|
| Built a FastAPI app with `prometheus-fastapi-instrumentator` | Auto-generates Prometheus metrics |
| Added logging at different levels | Creates meaningful log data for Loki |
| Exposed `/metrics` endpoint | Standard way for Prometheus to collect metrics |
| Viewed logs with `docker logs` | Understand what your app is reporting |
| Explored `/docs` endpoint | FastAPI's built-in API documentation |

Your application now **supports telemetry**—it actively reports on its own health, performance, and behavior. This is the foundation of observability!

---

## Challenge

1. **Add a custom metric** - Track how many items are in the database
2. **Add structured JSON logging** - Modify the logger to output JSON format
3. **Create a `/ready` endpoint** - Return 503 if the app isn't ready (simulate startup delay)
4. **Add request IDs** - Log a unique ID with each request for tracing

---

## Cleanup

```bash
docker stop telemetry-api
docker rm telemetry-api
```

---

## Next Lab

In the next lab, you'll deploy **Loki** to collect and query logs from this API!
