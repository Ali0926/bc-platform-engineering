# Day 3 Final Lab: Automation Gateway API (with Business Metrics)

## Scenario

Your platform team needs a **centralized Automation Gateway API** that other internal systems can call over REST.

Instead of every team building ad‑hoc scripts, we will expose a single API that:

- **Triggers automation actions** (today: simulated actions; later: AWX/Jenkins/GitHub Actions/etc.).
- **Tracks job runs** and returns job status.
- **Exports custom Prometheus metrics** that reflect *our business processes* (not just tool health).

By the end of this lab, you will deploy your own API into the cluster using the **Argo CD “app of apps”** pattern and prove it works via metrics in Grafana.

---

## What You’re Given

- A working EKS cluster with the **LGTM stack** already installed.
- A **reference FastAPI app** you can copy patterns from (logging, OpenTelemetry, DB usage, configuration, etc.).
- Argo CD bootstrapped with an **apps-of-apps** layout.

---

## Your Goal

Create and deploy a new service called:

> **automation-gateway**

It must be deployed via **Argo CD**, and it must expose:
- Required REST endpoints
- Required custom Prometheus metrics
- (Optional) logging / OpenTelemetry / DB integration improvements from your prior API work

---

## Part 1: Build the API (FastAPI)

### Requirements

Your API must implement the following endpoints.

#### 1) Health
- `GET /healthz`
  - Returns `200 OK`
  - Body: `{ "status": "ok" }`

#### 2) Create a Job Run
- `POST /v1/jobs`
  - Accepts JSON:
    ```json
    {
      "action": "restart_service",
      "target": "demo-api",
      "requested_by": "student@example.com",
      "parameters": {
        "service": "demo-api"
      }
    }
    ```
  - Returns `202 Accepted` with:
    ```json
    {
      "job_id": "<uuid-or-int>",
      "status": "PENDING"
    }
    ```

Supported actions (minimum):
- `restart_service`
- `rotate_config`
- `health_check`

> You can simulate the “automation” for now (sleep, random failures, mocked responses), but implement it as **asynchronous job execution** so that the API remains responsive.

#### 3) Get Job Status
- `GET /v1/jobs/{job_id}`
  - Returns:
    ```json
    {
      "job_id": "...",
      "action": "...",
      "target": "...",
      "status": "PENDING|RUNNING|SUCCESS|FAILED",
      "started_at": "...",
      "finished_at": "...",
      "error": "..."
    }
    ```

#### 4) Metrics
- `GET /metrics`
  - Exposes Prometheus metrics (see below).
  - Must be compatible with Prometheus scraping.

---

## Part 2: Implement Custom Prometheus Metrics

You must emit **business-process metrics** for automation runs.

### Required Metrics (Minimum)

> Use the Prometheus client library for Python.

#### A) Job counter
**Metric name:** `automation_jobs_total`

- Type: **Counter**
- Labels:
  - `action`
  - `status` (SUCCESS / FAILED)

Example time series:
- `automation_jobs_total{action="restart_service",status="SUCCESS"} 12`

#### B) Job duration histogram
**Metric name:** `automation_job_duration_seconds`

- Type: **Histogram**
- Labels:
  - `action`

This powers percentile dashboards (p95/p99).

#### C) Queue depth gauge
**Metric name:** `automation_job_queue_depth`

- Type: **Gauge**
- Meaning: number of jobs currently in `PENDING` state (or `PENDING + RUNNING` if you prefer—document your choice).

---

## Part 3: Deploy with the “Apps of Apps” Pattern (Argo CD)

### Repo Layout Requirements

In your fork, create:

1) A folder for your app manifests:
- `apps/automation-gateway/`

2) An Argo CD Application manifest under:
- `apps/argocd-apps/`

### What to put in `apps/automation-gateway/`

Add Kubernetes manifests for at least:

- **Deployment**
  - Runs your container image
  - Has reasonable CPU/memory requests/limits
  - Exposes container port (example: 8000)
- **Service**
  - ClusterIP service exposing the app
- **ServiceMonitor** (if your monitoring stack supports it)
  - Scrapes `/metrics`
  - Uses the correct labels/selector for your service
  - (If you are not using ServiceMonitor in your cluster, document the alternative scrape method.)

> Keep it simple. YAML you can explain beats YAML you copied.

### What to put in `apps/argocd-apps/`

Create a new Argo CD Application for your service, for example:

- Name: `automation-gateway`
- Points to path: `apps/automation-gateway`
- Targets the namespace you choose (or the existing “apps” namespace if your repo uses one)

---

## Part 4: Validate End-to-End

### Required Checks

#### 1) App is running
- `kubectl get deploy,po,svc -n <namespace>`

#### 2) Endpoints work
- Port-forward and test:
  - `GET /healthz`
  - `POST /v1/jobs`
  - `GET /v1/jobs/{job_id}`

#### 3) Metrics scrape
- Confirm Prometheus is scraping your metrics:
  - Your time series exist:
    - `automation_jobs_total`
    - `automation_job_duration_seconds_bucket` (histograms produce bucket/count/sum)
    - `automation_job_queue_depth`

#### 4) Grafana proof
Create a small dashboard (minimum 3 panels):

1. **Jobs per minute**
   - `sum(rate(automation_jobs_total[5m])) by (action, status)`

2. **p95 duration by action**
   - `histogram_quantile(0.95, sum(rate(automation_job_duration_seconds_bucket[5m])) by (le, action))`

3. **Queue depth**
   - `automation_job_queue_depth`

---

## Stretch Goals (Choose Any)

Add features you already built in previous APIs:

- Structured logging (JSON logs with `job_id`, `action`, `target`)
- OpenTelemetry tracing (trace the job lifecycle: request → enqueue → execute → finish)
- Persist job runs to a database table instead of in-memory storage
- Add `/v1/jobs` list endpoint with filtering by status/action
- Add an alert:
  - Success rate < 90% for 10 minutes
  - Queue depth > N for 10 minutes

---

## Success Criteria (How You’ll Be Graded)

✅ **Pass**
- Deployed via Argo CD (apps-of-apps)
- Required endpoints implemented
- Required metrics present and scraped
- Grafana dashboard with the 3 required panels

⭐ **Strong Pass**
- Clean async job execution (not blocking requests)
- Good logs/traces
- Job data persisted in DB
- Alerts + Slack integration

---

## Why We’re Doing This

This API becomes a **central control plane** for platform operations.

Later, you can replace the “simulated automation” with real integrations (AWX, Jenkins, GitHub Actions), while keeping:

- stable REST endpoints
- stable business metrics
- stable dashboards and alerts

Your platform stays observable and consistent even when tools change.

---

## Deliverables

1) Git changes in your fork:
- `apps/automation-gateway/*`
- `apps/argocd-apps/automation-gateway.yaml` (or similar)

2) Screenshot(s) or notes showing:
- Argo CD Application is Healthy/Synced
- Grafana dashboard panels populated

---

## Notes

- Keep endpoint names and metric names **exact**.
- If you make improvements or changes (like status labels), document them in your README or in code comments.
- If you get stuck, reduce scope: ship the minimum working version first.

